<!DOCTYPE html>




<html class="theme-next muse" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="xaHWbGrH27PtIRsHQwRbAzWeQmtdbVP8Sj8IoFGMFhA" />














  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  
    
      
    

    
  

  
    
    
    <link href="https://fonts.loli.net/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic|Roboto Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png?v=5.1.4">


  <link rel="mask-icon" href="/safari-pinned-tab.svg?v=5.1.4" color="#222">


  <link rel="manifest" href="/site.webmanifest">


  <meta name="msapplication-config" content="/browserconfig.xml" />



  <meta name="keywords" content="大数据,数据库/HBase,数据库/KV," />





  <link rel="alternate" href="/atom.xml" title="扔掉笔记 ᐛ" type="application/atom+xml" />






<meta name="description" content="HBase 架构简介 HBase包含3个重要组件：Zookeeper、HMaster和 HRegionServer。  （1）Zookeeper： 为整个 HBase集群提供协助服务，包括： a. 存放整个 HBase集群的元数据以及集群的状态信息。 b. 实现 HMaster主从节点的 failover。     ZooKeeper为 HBase集群提供协调服务，它管理着 HMaster和 HR">
<meta name="keywords" content="大数据,数据库&#x2F;HBase,数据库&#x2F;KV">
<meta property="og:type" content="article">
<meta property="og:title" content="HBase-03架构和原理">
<meta property="og:url" content="https://beefyheisenberg.github.io/32.Database/HBase-03架构和原理/index.html">
<meta property="og:site_name" content="扔掉笔记 ᐛ">
<meta property="og:description" content="HBase 架构简介 HBase包含3个重要组件：Zookeeper、HMaster和 HRegionServer。  （1）Zookeeper： 为整个 HBase集群提供协助服务，包括： a. 存放整个 HBase集群的元数据以及集群的状态信息。 b. 实现 HMaster主从节点的 failover。     ZooKeeper为 HBase集群提供协调服务，它管理着 HMaster和 HR">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://beefyheisenberg.github.io/images/hbase/hbase_arch.png">
<meta property="og:image" content="https://beefyheisenberg.github.io/images/hbase/hbase-memstore-cache.png">
<meta property="og:image" content="https://beefyheisenberg.github.io/images/hbase/HBase-RS-Storage-Machinery.png">
<meta property="og:image" content="https://beefyheisenberg.github.io/images/HBase-Region-WAL.png">
<meta property="og:image" content="https://beefyheisenberg.github.io/images/hbase/hbase-memstore-skiplist-chunck.png">
<meta property="og:image" content="https://beefyheisenberg.github.io/images/hbase-hfile-v1-1.png">
<meta property="og:image" content="https://beefyheisenberg.github.io/images/hbase-hfile-v1-2.png">
<meta property="og:image" content="https://beefyheisenberg.github.io/images/hbase/hbase_hfile_tree.png">
<meta property="og:image" content="https://beefyheisenberg.github.io/images/hbase/hbase-hfile-data-index.png">
<meta property="og:image" content="https://beefyheisenberg.github.io/images/HBase-DataBlock-KeyValue.png">
<meta property="og:image" content="https://beefyheisenberg.github.io/images/HBase-BloomFilter-Hash.png">
<meta property="og:image" content="https://beefyheisenberg.github.io/images/HFile-Compact.png">
<meta property="og:updated_time" content="2024-03-19T06:19:12.273Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="HBase-03架构和原理">
<meta name="twitter:description" content="HBase 架构简介 HBase包含3个重要组件：Zookeeper、HMaster和 HRegionServer。  （1）Zookeeper： 为整个 HBase集群提供协助服务，包括： a. 存放整个 HBase集群的元数据以及集群的状态信息。 b. 实现 HMaster主从节点的 failover。     ZooKeeper为 HBase集群提供协调服务，它管理着 HMaster和 HR">
<meta name="twitter:image" content="https://beefyheisenberg.github.io/images/hbase/hbase_arch.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://beefyheisenberg.github.io/32.Database/HBase-03架构和原理/"/>





  <title>HBase-03架构和原理 | 扔掉笔记 ᐛ</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">扔掉笔记 ᐛ</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">dropNotes</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      


    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://beefyheisenberg.github.io/32.Database/HBase-03架构和原理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="beefyheisenberg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/hexo_avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="扔掉笔记 ᐛ">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">HBase-03架构和原理</h1>
        

        <div class="post-meta">
          <span class="post-time">
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/32-Database/" itemprop="url" rel="index">
                    <span itemprop="name">32.Database</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  9,721
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="HBase-架构简介"><a href="#HBase-架构简介" class="headerlink" title="HBase 架构简介"></a>HBase 架构简介</h2><p><img src="/images/hbase/hbase_arch.png" alt=""></p>
<p>HBase包含3个重要组件：Zookeeper、HMaster和 HRegionServer。</p>
<ul>
<li>（1）Zookeeper： 为整个 HBase集群提供协助服务，包括：<ul>
<li>a. 存放整个 HBase集群的元数据以及集群的状态信息。</li>
<li>b. 实现 HMaster主从节点的 failover。</li>
</ul>
</li>
</ul>
<blockquote>
<p>ZooKeeper为 HBase集群提供协调服务，它管理着 HMaster和 HRegionServer的状态(available/alive等)，并且会在它们宕机时通知给 HMaster，从而 HMaster可以实现 HMaster之间的 failover，或对宕机的 HRegionServer中的 HRegion集合的修复(将它们分配给其他的 HRegionServer)。</p>
</blockquote>
<ul>
<li><p>（2）HMaster： 主要用于监控和操作集群中的所有 HRegionServer。HMaster没有单点问题，HBase中可以启动多个HMaster，通过 Zookeeper的 MasterElection机制保证总有一个Master在运行，HMaster 主要负责 Table和 Region的管理工作：</p>
<ul>
<li>a. 管理用户对表的增删改查操作</li>
<li>b. 管理 HRegionServer的负载均衡，调整Region分布</li>
<li>c. Region Split后，负责新 Region的分布</li>
<li>d. 在 HRegionServer停机后，负责失效 HRegionServer上Region迁移</li>
</ul>
</li>
<li><p>（3）HRegion Server： HBase中最核心的模块，主要负责响应用户I/O请求，向HDFS文件系统中读写数据。</p>
<ul>
<li>a. 存放和管理本地 HRegion。</li>
<li>b. 读写HDFS，管理Table中的数据。</li>
<li>c. Client直接通过 HRegionServer读写数据（从HMaster中获取元数据，找到RowKey所在的 HRegion/HRegionServer后）。</li>
</ul>
</li>
</ul>
<h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><p>底层的存储都是基于 Hadoop HDFS 的：</p>
<ul>
<li><p>Hadoop NameNode 维护了所有 HDFS 物理 Data block 的元信息。</p>
</li>
<li><p>Hadoop DataNode 负责存储 Region Server 所管理的数据。所有的 HBase 数据都存储在 HDFS 文件中。Region Server 和 HDFS DataNode 往往是一起部署的，这样 Region Server 就能够实现数据本地化（data locality）</p>
</li>
</ul>
<p>在 Region 的监控页面，可以看到一个监控指标 “<strong>Locality（本地化率）</strong>”，是一个百分比，表示 region 管理的数据即位于本地的百分比（“本地”指：数据所在的 HDFS DataNode 和 Region Server 在一个节点 ）</p>
<p>本地化率低产生的问题：因为大量 HFile 数据和 Region 不在同一个节点上，会产生大量的网络 IO，导致读请求延迟较高；</p>
<p>引起 RegionServer 数据本地化率低的几种可能原因：</p>
<ul>
<li>1) Balance 引起的：当数据持续写入，单个 region 的大小达到 hbase.hregion.max.filesize(默认值10GB)会自动进行 Split,假如一直向 RegionA 持续写入数据，当 RegionA 大小超过10GB，会分离成两个子 RegionB、RegionC，如果我们集群开启了负载均衡，当前节点 Region 比较多，其他节点 Region 数量少的时候就会把 RegionB 或 RegionC 迁移到 Region 相对比较少的节点上去。</li>
<li>2) RegionServer 宕机，手动 move 使 region 迁移到其他节点，导致数据本地化率降低。</li>
</ul>
<p><strong>Region 迁移如何导致 Locality 下降</strong>：</p>
<blockquote>
<p>在 hadoop 生产环境中， hdfs 通常为设置为三个副本，假如当前 RegionA 处于 datanode1，<br>当数据 a 通过从 Memstore 中 Flush 到 HFile 时,三副本分别在(datanode1,datanode2,datanode3),<br>数据 b 写入三副本：(datanode1,datanode3,datanode5),<br>数据 c 写入三副本：(datanode1,datanode4,datanode6),<br>可以看出来 a、b、c 三份数据写入的时候肯定会在本地 datanode1上写入一份，其他两份按照 datanode 的写入机制进行分配;数据都在本地可以读到，因此数据本地率是100%。<br>现在假设 RegionA 被迁移到了 datanode2上，只有数据 a 有一份数据在该节点上，其他数据(b 和 c)读取只能远程跨节点读，本地率就为33% (假设 a，b 和 c 的数据大小相同)。</p>
</blockquote>
<p>如何提高 RegionServer 数据的本地化率：</p>
<ul>
<li>RegionServer 重启后，手动 move 到原来节点（如果生产 region 比较多，这个操作比较繁琐）</li>
<li>定期执行 major compaction（尤其对于 Locality 低的 RegionServer，如果不做 major compaction 会非常影响读写性能 ）</li>
</ul>
<p>@ref: <a href="https://product.hubspot.com/blog/healing-hbase-locality-at-scale" target="_blank" rel="noopener">Healing HBase Locality At Scale</a></p>
<p>HDFS 三副本机制 @link [[../33.Bigdata/Hadoop#副本机制]]</p>
<blockquote>
<ul>
<li>1）第一个副本块保存在 HDFS Client 同一个的 DataNode ；</li>
<li>2）第二个副本块保存在 HDFS Client 所在 DataNode 同机架的其他 DataNode ；</li>
<li>3）第三个副本块保存不同机架的某个 DataNode ；</li>
</ul>
</blockquote>
<h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><p>Zookeeper 用来协调分布式系统中集群状态信息的共享。Region Servers 和在线 HMaster（active HMaster）和 Zookeeper 保持会话（session）。Zookeeper 通过心跳检测来维护所有<strong>临时节点</strong>（ephemeral nodes）。</p>
<p>每个 Region Server 都会创建一个 ephemeral 节点。HMaster 会监控这些节点来发现可用的 Region Servers，同样它也会监控这些节点是否出现故障。</p>
<p>HMaster 们会竞争创建 ephemeral 节点，而 Zookeeper 决定谁是第一个作为在线 HMaster，保证线上只有一个 HMaster。在线 HMaster（<strong>active HMaster</strong>） 会给 Zookeeper 发送心跳，不在线的待机 HMaster （<strong>inactive HMaster</strong>） 会监听 active HMaster 可能出现的故障并随时准备上位。</p>
<p>如果有一个 Region Server 或者 HMaster 出现故障或各种原因导致发送心跳失败，它们与 Zookeeper 的 session 就会过期，这个 ephemeral 节点就会被删除下线，监听者们就会收到这个消息。Active HMaster 监听的是 region servers 下线的消息，然后会恢复故障的 region server 以及它所负责的 region 数据。而 Inactive HMaster 关心的则是 active HMaster 下线的消息，然后竞争上线变成 active HMaster。</p>
<h2 id="META-表"><a href="#META-表" class="headerlink" title="META 表"></a>META 表</h2><p>当 Client 发起的每个 RPC 请求，实际上都需要发送到对应的 RegionServer 上执行，所以第一步是通过 Table 和 Rowkey <strong>定位 Region</strong> ，对应关系存储在 <code>hbase:meta</code> 表：</p>
<p><code>hbase:meta</code> 是一个比较特殊的 hbase 表，也叫<strong>元数据表</strong>（不能切分，只保存在一台 RegionServer 上）</p>
<p>在0.96之后元数据的查询模型由3层结构变成2层，即：</p>
<ul>
<li>首先查询 zk 获取 hbase:meta 所在的 regionserver；</li>
<li>然后去对应的 RegionServer 上查询表对应的 region 信息。region 信息中会包含开始的 rowkey，regionserver 的地址等信息。</li>
</ul>
<p><code>hbase:meta</code> 表中的一个 Rowkey 就代表了一个 region。Rowkey 主要由以下几部分组成：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">table_name,start_rowkey,create_timestamp.EncodedName.</span><br><span class="line"></span><br><span class="line">table_name: 表名</span><br><span class="line"></span><br><span class="line">start_rowkey 是table 在 Region上的 起始rowkey，为空的，说明这是该table的第一个region。若对应region中startkey为空的话，表明这个table只有一个region；</span><br><span class="line"></span><br><span class="line">create_timestamp ： region 创建时间；</span><br></pre></td></tr></table></figure>
<p>meta 表只有一个列簇 info，并且包含四列：  </p>
<ol>
<li>regioninfo ：当前 region 的 startKey 与 endKey，name 等 </li>
<li>seqnumDuringOpen：  </li>
<li>server：region 所在服务器及端口  </li>
<li>serverstartcode：服务开始的时候的 timestamp</li>
</ol>
<p>为了避免每次 Client 执行 RPC 都要查询 Zk，Client 会缓存 meta 的数据：</p>
<blockquote>
<p><strong>Meta 缓存</strong>的数据结构采用了 copy on write 的思想，自定义了一个 CopyOnWriteArrayMap。<br>copy on write 即可以支持并发读，当写的时候采用拷贝引用的方式快速变更。<br>HBase 自定义了一个数组 Map，其中数组结构第一层为表，数组部分的查询采用二分查找；第二层是 startkey；当有 RegionLocation 信息需要更新时，采用 System.arraycopy 实现快速拷贝更新。</p>
</blockquote>
<p>图: 通过 table 和 rowkey 快速定位到对应的 Region:<br><img src="/images/hbase/hbase-memstore-cache.png" alt="hbase_memstore_cache"></p>
<p>@ref: </p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/92103602" target="_blank" rel="noopener">HBase源码：Region的定位与优化 - 知乎</a></li>
<li><a href="https://www.jianshu.com/p/ea7e03cfbfe0" target="_blank" rel="noopener">HBase meta表介绍 - 简书</a></li>
<li><a href="https://segmentfault.com/a/1190000040267826" target="_blank" rel="noopener">HBase Meta表结构组成 - 个人文章 - SegmentFault 思否</a></li>
</ul>
<h2 id="HMaster"><a href="#HMaster" class="headerlink" title="HMaster"></a>HMaster</h2><p>HMaster 负责 Region 的分配，DDL（创建，删除表）等操作：</p>
<p>统筹协调所有 region server：</p>
<ul>
<li>启动时分配 regions，在故障恢复和负载均衡时<strong>重分配</strong> regions </li>
<li>监控集群中所有 Region Server 实例（从 Zookeeper 获取通知信息）</li>
</ul>
<p>管理员功能：</p>
<ul>
<li>提供创建，删除和更新 HBase Table 的接口</li>
</ul>
<h2 id="HRegion-Server"><a href="#HRegion-Server" class="headerlink" title="HRegion Server"></a>HRegion Server</h2><p>RegionServer:<br><img src="/images/hbase/HBase-RS-Storage-Machinery.png" alt=""></p>
<p>➤ HRegion Server 的构成：</p>
<ul>
<li><p><strong>BlockCache</strong>: Region Server 的读缓存。保存使用最频繁的数据，使用 <strong>LRU</strong> 算法换出不需要的数据;</p>
</li>
<li><p><strong>HLog</strong>: WAL(Write-Ahead-Log), 为数据提供 Crash-Safe, 以及读一致性及 undo/redo 回滚等数据恢复操作;</p>
</li>
<li><p><strong>HRegion</strong>: 即<strong>子表</strong>, 每个子表都关联一个 <code>[StartKey, EndKey]</code> 的存储区间, 每个 HRegion Server 管理多个”子表”;</p>
</li>
<li><p><strong>HStore</strong>: 每个 Region 包括多个 HStore, 每个 HStore 由 <code>1 * MemStore</code> + <code>n * StoreFile</code> 组成 (LSM-Tree 的 C0..Ck 层)</p>
<ul>
<li><strong>MemStore</strong>: LSM-Tree的 C0层, 存储于内存的有序K-V结构, 使用<code>ConcurrentSkipList</code>实现, 当 MemStore（默认 64MB）写满之后，会开始 flush 到磁盘上的 StoreFile</li>
<li><strong>StoreFile（HFile）</strong>: LSM-Tree 的 Ck 层, StoreFile 是对 HFile 做了一层简单封装</li>
</ul>
</li>
</ul>
<p>➤ 从逻辑存储看 HRegion Server:</p>
<ul>
<li>HBase 表的逻辑存储: Table, Family, Qualifier</li>
<li>每个 HRegion 存储一张 Table 的一段 Key 区间</li>
<li>每个 HStore 存储一个 Family(列族)的一段 Key 区间</li>
</ul>
<p>➤ HBase 如何实现 LSM-Tree:</p>
<ul>
<li>每个 HStore 都有1个 MemStore (C0层) 和 N 个 StoreFile (C1..Ck 层)</li>
<li>MemStore: 跳表实现, MemStore 的数据超过阈值(默认64MB)后会刷写到磁盘, 生成 StoreFile</li>
<li>StoreFile(HFile): HBase 会自动合并一些小的 HFile，重写成少量更大的 HFiles。这个过程被称为 <strong>Minor Compaction</strong>。它使用归并排序算法，将小文件合并成大文件，有效减少 HFile 的数量。<strong>Major Compaction</strong> 合并重写每个 Column Family 下的所有的 HFiles，成为一个单独的大 HFile，在这个过程中，被删除的和过期的 cell 会被真正从物理上删除，这能提高读的性能。</li>
</ul>
<p>有关LSM-Tree, 参考: <a href="/32.Database/LSM-Tree理论基础/" title="LSM-Tree理论基础">LSM-Tree理论基础</a></p>
<h2 id="HLog-WAL"><a href="#HLog-WAL" class="headerlink" title="HLog (WAL)"></a>HLog (WAL)</h2><p>HLog 是 WAL(Write-Ahead-Log)的具体实现，数据的更新首先被写入 HLog，在 Region Server 宕机时可以通过回放 Hlog 实现 Crash-safe</p>
<p>HLog 实现细节：</p>
<ul>
<li>每个 Region Server 可以有一个 or 多个 HLog 文件（默认只有1个，1.x 版本可以开启 MultiWAL 功能，允许多个 HLog） // @doubt: 这几个 HLog 文件是顺序的？</li>
<li>一个 HLog 是被多个 Region（子表）共享的，也即当前所有子表的写入，都落到一个 HLog 文件中；</li>
<li>HLog 中，一个基本数据单元为 <strong>HLogKey</strong> + <strong>WALEdit</strong><ul>
<li>HLogKey：包含 table name、region name、sequenceid 等</li>
<li>WALEdit：WALEdit用来表示一个事务中的更新集合，一次行级事务可以原子操作同一行中的多个列。上图中WALEdit包含多个KeyValue</li>
</ul>
</li>
<li>由上可知，HLog 中每个基本单元代表一次事务中的所有更新集合，HLogKey 中的 sequenceid 是单增整数，可以认为是一次<strong>行级事务</strong>的自增 ID</li>
<li>HBase 为每个 Region （实际是每个 Store，对应某个列族）维护了一个 oldestUnflushedSequenceId，最新未落盘的 sequenceid，每次 MemStore flush 到 HFlie 后，更新每个 Region 的 oldestUnflushedSequenceId</li>
<li>在 Region Server 的情况下，HLog 中哪些日志需要回放，哪些需要 skip？ 因为 Region Server 已经宕机，所以 Region 对应的 oldestUnflushedSequenceId 也无法获取，实际上每次 HFile 落盘时，会把 oldestUnflushedSequenceId 以元数据的形式写入 HFile，所以在宕机迁移时，只需要重新读取 HFile 中的元数据即可恢复出这个核心变量oldestUnflushedSequenceId</li>
</ul>
<p>下图是中三个 Region（A、B、C）共享一个 HLog，日志最小单元是 HlogKey + WALEdit：<br><img src="/images/HBase-Region-WAL.png" alt="../_images/HBase-Region-WAL.png"></p>
<p>@ref: <a href="http://hbasefly.com/2017/07/02/hbase-sequenceid/" target="_blank" rel="noopener">HBase原理－要弄懂的sequenceId – 有态度的HBase/Spark/BigData</a></p>
<h2 id="MemStore"><a href="#MemStore" class="headerlink" title="MemStore"></a>MemStore</h2><ul>
<li>一个子表可能包括多个列族，即一个 Region 可能包括多个 <strong>MemStore</strong>；</li>
<li><strong>MemStore</strong> 和列族是一一对应的；</li>
<li>MemStore 基于跳表实现，元素通过 KeyValue 对象的 key 进行排序，而不简单的通过 Rowkey 排序；</li>
<li>当 <strong>MemStore</strong>（默认 64MB）写满之后，会 flush 到 HDFS 上生成 StoreFile，<strong>MemStore</strong> 相当于 LSM-Tree 结构中的 C0层，这一步叫做 <strong>Region flush</strong> ；</li>
</ul>
<p>➤ KV 在 MemStore 中的排序：</p>
<p>构成 HBase 的 KV 在同一个文件中都是有序的，但规则并不是仅仅按照 rowkey 排序，而是按照 KV 中的 key 进行排序——先比较 rowkey，rowkey 小的排在前面；如果 rowkey 相同，再比较 column，即 column family:qualifier，column 小的排在前面；如果 column 还相同，再比较时间戳 timestamp，即版本信息，timestamp 大的排在前面。KV 结构参考 「HFile」 一节</p>
<p>➤ 为什么不建议使用过多的列族：</p>
<p>Flush 操作时 Region 级别的（也就是为什么叫 Region Flush 而不是 “MemStore Flush”），Region 中某个 Memstore 被 Flush，同一个 Region 的其他 Memstore 也会进行 Flush 操作。<br>当表有很多列族，且列族之间数据不均匀，例如一个列族有100W 行，一个列族只有10行，列族1很容易触发 Flush，列族2虽然数据不多但也需要进行 Flush，而且每次 Flush 操作都涉及到一定的磁盘 IO。<br>例如基于 HBase 的 OpenTSDB，存放数据的表只有一个列族。</p>
<p>过多的列族也即意味着更多的 MemStore，占用更多的 JVM 内存。</p>
<p>➤ MemStore 的内部实现：</p>
<p>HBase 的 MemStore 基于 <code>ConcurrentSkipListMap</code> 跳表实现，由于 KeyValue 是存储于内存中的，对于很多写入吞吐量几万每秒的业务来说，每秒就会有几万个内存对象产生，这些对象会在内存中存在很长一段时间，对应的会晋升到老生代，一旦执行了 flush 操作，老生代的这些对象会被 GC 回收掉，导致 JVM 的 GC 压力非常大。</p>
<p>GC 压力主要来源于：这些内存对象一旦晋升到老生代，执行完 OldGen GC 后会存在大量的非常小的内存碎片（例如老年代的回收器 CMS，是不带内存压缩的），这些内存碎片会引起频繁的 Full GC，而且每次 Full GC 的时间会异常的长。</p>
<p>➤ 一个 KV 在 MemStore 中的写入过程：</p>
<ol>
<li>在 ChunkPool 中申请一个 Chunk（2M），在 Chunk 中分配一段与 KV 相同大小的内存空间将 KV 拷贝进去。一旦 Chunk 写满，再申请下一个 Chunk;</li>
<li>待插入的 KV 生成一个 Cell 对象，该对象包含指向 Chunk 中对应数据块的指针、offsize 以及 length;</li>
<li>将这个 Cell 对象分别作为 Key 和 Value 插入到 <code>ConcurrentSkipListMap</code> 跳表中；</li>
</ol>
<p><img src="/images/hbase/hbase-memstore-skiplist-chunck.png" alt="hbase_memstore_skiplist_chunck"></p>
<p>HBase 在 MemStore 设计中的改进 @ref : <a href="http://hbasefly.com/2019/10/18/hbase-memstore-evolution/" target="_blank" rel="noopener">HBase内存管理之MemStore进化论 – 有态度的HBase/Spark/BigData</a></p>
<h2 id="HFile"><a href="#HFile" class="headerlink" title="HFile"></a>HFile</h2><h3 id="V1-V2的改进"><a href="#V1-V2的改进" class="headerlink" title="V1-V2的改进"></a>V1-V2的改进</h3><p>HFile 是 HBase 存储数据的文件组织形式，参考 BigTable 的 SSTable 和 Hadoop 的 TFile 实现。</p>
<p>从 HBase 开始到现在，HFile 经历了<strong>三个版本</strong>，其中 V2在0.92引入，V3在0.98引入。<br>HFileV1版本的在实际使用过程中发现它占用内存多，HFile V2版本针对此进行了优化，HFile V3版本基本和 V2版本相同，只是在 cell 层面添加了 Tag 数组的支持。</p>
<p>➤ V1的 HFile，参考了 Bigtable 的 SSTable 以及 Hadoop 的 TFile(HADOOP-3315)：</p>
<p><img src="/images/hbase-hfile-v1-1.png" alt="../_images/hbase-hfile-v1-1.png"></p>
<ul>
<li>Data Block：每个 Data 块默认为<strong>64KB</strong>，存储 KV 数据，由于 KV 数据在 MemStore 已经进行排序，所以 Data Block 中的 KV 也是有序的</li>
<li>Data Block Index：这部分存储了每一个 Data Block 的索引信息 {Offset，Size，FirstKey}</li>
<li>Meta Block：主要是 <strong>Bloom Filter</strong>，用来快速定位 Key 是否在 HFile</li>
<li>Meta Block Index： 每个 Meta Block 的索引</li>
<li><p>Tailer：存储了上面几个段的索引，例如 Data Block Index 的索引信息，{Data Index Offset, Data Block Count}</p>
<p>Bloom filter 主要用来快速定位 Key 是否在 HFile。Bloom Block 的数据是在启动的时候就已经加载到内存里，除了 Block Cache 和 MemStore 以外，这个也对 HBase 随机读性能的优化起着至关重要的作用。<br>生成 HFile 的时候，会将 key 经过三次 hash 最终落到 Bloom Block 位数组的某三位上，并将其由0更改成1，以此标记该 key 的确存在这个 HFile 文件之中，查询的时候不需要将文件打开并检索，避免了一次 I/O 操作。然而随着 HFile 的膨胀，Bloom Block 会越来越大。</p>
</li>
</ul>
<blockquote>
<p>HDFS 中定义的 Block size 是 <strong>64~128MB</strong>，HFile 中的 Block 大小 = <strong>64KB</strong>，区别 Linux 文件系统（Ext4等）定义的块大小 = <strong>4KB</strong> ；</p>
</blockquote>
<p>下图更直观的表达出了 HFile V1中的数据组织结构：</p>
<p><img src="/images/hbase-hfile-v1-2.png" alt="../_images/hbase-hfile-v1-2.png"></p>
<p>V1 的设计简单、直观。但缺点也很明显，HFile 中的 Bloom Filter 包括了该 HFile 中所有的 KV，加载一个 HFile 需要占用很大的内存来存储  Bloom Filter、Data Block Index。<br>Region Open 的时候，需要加载所有的 Data Block Index 数据，另外，第一次读取时需要加载所有的 Bloom Filter 数据到内存中。一个 HFile 中的 Bloom Filter 的数据大小可达百 MB 级别，一个 RegionServer 启动时可能需要加载数 GB 的 Data Block Index 数据。这在一个大数据量的集群中，几乎无法忍受。</p>
<p>➤ <strong>HFile V2 的改进</strong>：</p>
<p>在 V2中，无论是 Data Block Index 还是 Bloom Filter，都采用了<strong>分层索引</strong>的设计。</p>
<p>Data Block 的索引，在 HFile V2中做多可支持三层索引，每层的索引都在单独的 Block 中存储：Root Data Index -&gt; Intermediate Index Block -&gt; Leaf Index Block，<br>最底层的 Data Block Index 称之为 Leaf Index Block，可直接索引到 Data Block；</p>
<p>索引逻辑为：由 Root Data Index 索引到 Intermediate Block Index，再由 Intermediate Block Index 索引到 Leaf Index Block，最后由 Leaf Index Block 查找到对应的 Data Block。<br>在实际场景中，<strong>Intermediate Block Index 基本上不会存在</strong>，因此，索引逻辑被简化为：由 Root Data Index 直接索引到 Leaf Index Block，再由 Leaf Index Block 查找到的对应的 Data Block。</p>
<p>Root Data index 存放在一个称之为”<strong>Load-on-open Section</strong>“区域，在 Region Open 时会被加载到内存中，而 Intermediate &amp;  Leaf Index Block 则按需加载。</p>
<p>Bloom Block 也采用了分层设计，整个 HFile 的 Bloom Filter 也被拆成了多个 Bloom Block ，在”Load-on-open Section”区域中存放了所有 <strong>Bloom Block 的索引</strong>，而 Bloom Block （数据块）按需加载。</p>
<p><strong>总结</strong>：V2通过分层索引，无论是 Data Block 的索引数据，还是 Bloom Filter 数据，都被拆成了多个 Block 当中，可以<strong>按需读取</strong>，避免在 Region Open 阶段或读取阶段一次读入大量的数据，降低了阶段耗时，也解决了 V1内存占用高的问题。</p>
<p>图-HFile V2的 Block 组织，右边是 Data Index 的<strong>三层索引</strong>结构：</p>
<p><img src="/images/hbase/hbase_hfile_tree.png" alt="HFile结构-图2"></p>
<h3 id="Data-Block-的三层索引"><a href="#Data-Block-的三层索引" class="headerlink" title="Data Block 的三层索引"></a>Data Block 的三层索引</h3><p>➤ <strong>Data Block Index 的三层索引</strong>：<br><img src="/images/hbase/hbase-hfile-data-index.png" alt="hbase_hfile_data_index"></p>
<p>图中上面三层为索引层，在数据量不大的时候只有最上面一层，数据量大了之后开始分裂为多层，最多三层，如图所示。最下面一层为数据层，存储用户的实际 keyvalue数据。这个索引树结构类似于 InnoSQL的聚集索引，只是 HBase并没有辅助索引的概念。</p>
<p>图中红线表示一次查询的索引过程（HBase中相关类为 HFileBlockIndex和 HFileReaderV2），基本流程可以表示为：</p>
<ol>
<li>用户输入rowkey为fb，在root index block中通过二分查找定位到fb在’a’和’m’之间，因此需要访问索引’a’指向的中间节点。因为root index block常驻内存，所以这个过程很快。</li>
<li>将索引’a’指向的中间节点索引块加载到内存，然后通过二分查找定位到fb在index ‘d’和’h’之间，接下来访问索引’d’指向的叶子节点。</li>
<li>同理，将索引’d’指向的中间节点索引块加载到内存，一样通过二分查找定位找到fb在index ‘f’和’g’之间，最后需要访问索引’f’指向的数据块节点。</li>
<li>将索引’f’指向的数据块加载到内存，通过遍历的方式找到对应的keyvalue。</li>
</ol>
<p>上述流程中因为 <strong>中间节点</strong>、 <strong>叶子节点</strong> 和 <strong>数据块</strong> 都需要加载到内存，所以 IO 次数最大为3次。但是实际上 HBase 为 block 提供了缓存机制，可以将频繁使用的 block 缓存在内存中，可以进一步加快实际读取过程。所以，在 HBase 中，通常一次随机读请求最多会产生3次 io，如果数据量小（只有一层索引），数据已经缓存到了内存，就不会产生 io。</p>
<h3 id="Data-Block-的-KV-结构"><a href="#Data-Block-的-KV-结构" class="headerlink" title="Data Block 的 KV 结构"></a>Data Block 的 KV 结构</h3><p>➤ <strong>Data Block 的结构</strong>：</p>
<p>DataBlock 是 HBase 中数据存储的最小单元。DataBlock 中主要存储用户的 KeyValue 数据，KeyValue 结构在内存和磁盘中可以表示为：</p>
<p><img src="/images/HBase-DataBlock-KeyValue.png" alt="../_images/HBase-DataBlock-KeyValue.png"></p>
<p>可以看到，在 KeyValue 结构中，</p>
<ul>
<li>key 是一个复杂的结构，首先是 rowkey 的长度，接着是 rowkey，然后是 ColumnFamily 的长度，再是 ColumnFamily，之后是 ColumnQualifier，最后是时间戳和 KeyType（keytype 有四种类型，分别是 Put、Delete、 DeleteColumn 和 DeleteFamily）</li>
<li>value 就没有那么复杂，就是一串纯粹的二进制数据</li>
</ul>
<h3 id="BloomFilter"><a href="#BloomFilter" class="headerlink" title="BloomFilter"></a>BloomFilter</h3><p>#BloomFilter 对于 HBase 的随机读性能至关重要，对于 get 操作以及部分 scan 操作可以剔除掉不会用到的 HFile 文件，减少实际 IO 次数，提高随机读性能。</p>
<p>BloomFilter 使用 bit 数组实现，KeyValue 在写入 HFile 时，会经过3个 Hash 函数，生成3个不同的 hash 值，映射到 bit 数组上3个不同的位置，接下来将对应的 bit 置为1；</p>
<p>下图中，两个元素 x 和 y，分别被3个 hash 函数进行映射，映射到的位置分别为（0，3，6）和（4，7，10），对应的位会被置为1:</p>
<p><img src="/images/HBase-BloomFilter-Hash.png" alt="../_images/HBase-BloomFilter-Hash.png"></p>
<p>查找时，元素要进行3次 hash 映射，如果3个位置上都是1，那么元素可能存在于这个 DataBlock；<br>如果有一个位置是0，那么元素肯定不存在于这个 DataBlock；</p>
<hr>
<p>@ref:</p>
<ul>
<li><a href="https://bbs.huaweicloud.com/blogs/133730" target="_blank" rel="noopener">HBase高性能随机查询之道 – HFile原理解析-云社区-华为云</a></li>
<li><a href="http://hbasefly.com/2016/03/25/hbase-hfile/" target="_blank" rel="noopener">HBase – 存储文件HFile结构解析 – 有态度的HBase/Spark/BigData</a></li>
<li><a href="http://hbasefly.com/2016/04/03/hbase_hfile_index/" target="_blank" rel="noopener">HBase – 探索HFile索引机制 – 有态度的HBase/Spark/BigData</a></li>
</ul>
<h2 id="Minor-Major-Compaction"><a href="#Minor-Major-Compaction" class="headerlink" title="Minor/Major Compaction"></a>Minor/Major Compaction</h2><ul>
<li><p><strong>Minor Compaction</strong>： HBase 会自动合并一些小的 HFile，重写成少量且更大的 HFiles。这个过程被称为 <strong>Minor Compaction</strong>。它使用归并排序算法，将小文件合并成大文件，有效减少 HFile 的数量。</p>
</li>
<li><p><strong>Major Compaction</strong> ：合并每个 Column Family 下的<strong>所有的</strong> HFiles，生成<strong>一个大的 HFile</strong>，在这个过程中，被删除的和过期的 cell 会被真正删除。因为 major compaction 会重写所有的 HFile，会产生大量的硬盘 I/O 和网络开销。这被称为<strong>写放大</strong>（<strong>Write Amplification</strong>）。</p>
</li>
</ul>
<blockquote>
<p>默认情况下 Minor Compaction 也会删除数据，但只是删除合并 HFile 中的 TTL 过期数据。Major Compaction 是完全删除无效数据，包括被删除的数据、TTL 过期数据以及版本号过大的数据。</p>
</blockquote>
<p>下图直观描述了 Flush 与 Minor &amp; Major Compaction 流程：Region 级别的 Memstore flush 触发会触发 Minor Compaction 条件检查，符合条件则进行小合并，Minor Compaction 还可能触发 Major Compaction</p>
<p><img src="/images/HFile-Compact.png" alt="../_images/HFile-Compact.png"></p>
<p>Minor Compaction 的检查条件有三种：MemStore Flush、后台线程周期性检查、手动触发；</p>
<ul>
<li>MemStore flush：MemStore 的 size 达到一定阈值或其他条件时就会触发 flush 刷写到磁盘生成 HFile 文件， HFile 文件越来越多则触发 compact。HBase 每次 flush 之后，都会判断是否要进行 compaction，一旦满足 minor compaction 或 major compaction 的条件便会触发执行。</li>
<li>后台线程周期性检查： 后台线程 CompactionChecker 会定期检查是否需要执行 compaction，检查周期为 <code>hbase.server.thread.wakefrequency*hbase.server.compactchecker.interval.multiplier</code>，这里主要考虑的是一段时间内没有写入请求仍然需要做 compact 检查。其中参数 <code>hbase.server.thread.wakefrequency</code> 默认值 10000 即 10s，是 HBase 服务端线程唤醒时间间隔，用于 log roller、memstore flusher 等操作周期性检查；参数 <code>hbase.server.compactchecker.interval.multiplier</code> 默认值1000，是 compaction 操作周期性检查乘数因子。<code>10 * 1000 s</code> 时间上约等于2小时46分钟。</li>
<li>手动触发：是指通过 HBase Shell、Master Web UI 或者 HBase API 等任一种方式执行 compact、major_compact 等命令。</li>
</ul>
<p>触发 Minor Compaction 检查后，会根据下列参数决定哪些文件进入候选队列：</p>
<ol>
<li><p><code>hbase.hstore.compaction.min</code>：一次 minor compaction 最少合并的 HFile 数量，默认值 3。表示至少有3个符合条件的 HFile，minor compaction 才会启动。一般情况下不建议调整该参数。</p>
</li>
<li><p><code>hbase.hstore.compaction.max</code>：一次 minor compaction 最多合并的 HFile 数量，默认值 10。这个参数也是控制着一次压缩的时间。一般情况下不建议调整该参数。调大该值意味着一次 compaction 将会合并更多的 HFile，压缩时间将会延长。</p>
</li>
<li><p><code>hbase.hstore.compaction.min.size</code>：文件大小 ＜ 该参数值的 HFile 一定是适合进行 minor compaction 文件，默认值 128M（memstore flush size）。意味着小于该大小的 HFile 将会自动加入（automatic include）压缩队列。一般情况下不建议调整该参数。</p>
</li>
<li><p><code>hbase.hstore.compaction.max.size</code>：文件大小 ＞ 该参数值的 HFile 将会被排除，不会加入 minor compaction，默认值 Long.MAX_VALUE，表示没有限制。一般情况下也不建议调整该参数。</p>
</li>
<li><p><code>hbase.hstore.compaction.ratio</code>：这个 ratio 参数的作用是判断文件大小 ＞ hbase.hstore.compaction.min.size 的 HFile 是否也是适合进行 minor compaction 的，默认值1.2。</p>
</li>
</ol>
<p><strong>Major Compaction</strong> 是 Minor Compaction 升级而来的，检查周期同 Minor，Minor 首先对 HFile 进行筛选，是否进入候选队列，接下来会再判断是否满足 major compaction 条件，满足一条即进行 Major Compact：</p>
<ol>
<li>用户强制执行 major compaction</li>
<li>长时间没有进行 compact（见下面 <code>hbase.hregion.majorcompaction</code> 解析）且候选文件数小于 <code>hbase.hstore.compaction.max</code>（默认10）</li>
<li>Store 中含有 Reference 文件，Reference 文件是 split region 产生的临时文件，只是简单的引用文件，一般必须在 compact 过程中删除</li>
</ol>
<p>如何判断“长时间没有进行 compact” ？ 两次 major compactions 间隔 ＞ majorcompaction x majorcompaction.jitter：</p>
<ul>
<li><p><code>hbase.hregion.majorcompaction</code>：major Compaction 发生的周期，单位是毫秒，默认值是7天。</p>
</li>
<li><p><code>hbase.hregion.majorcompaction.jitter</code> ：0~1.0的一个指数。调整这个参数可以让 Major Compaction 的发生时间更灵活，默认值是0.5。</p>
</li>
</ul>
<blockquote>
<p>这个参数是为了避免 major compaction 同时在各个 regionserver 上同时发生，避免此操作给集群带来很大压力。这样节点 major compaction 就会在 + 或 - 两者乘积的时间范围内<strong>随机</strong>发生。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">@TLDR：Minor &amp; Major Compaction、Region split 检查周期、触发条件</span><br><span class="line"></span><br><span class="line">- Minor 检查的触发：Memstore flush（默认64M）会触发 minor compact 检查，此外 CompactionChecker 线程也会定期检查（≈ 2小时）</span><br><span class="line"></span><br><span class="line">- Minor 条件：根据 HFile 的大小决定是否进入 compact 队列，候选文件数过多则触发 minor compact；</span><br><span class="line"></span><br><span class="line">- Major 一般由 Minor 触发：如果此次 minor compact 进入候选文件数 ＜ `hbase.hstore.compaction.max` ，且距上次 major compact 足够的时间间隔（≈7day），则触发 major compact；</span><br><span class="line"></span><br><span class="line">- Region split：Region 中的 store 大小，超过阈值 `hbase.hregion.max.filesize`，切分对象是整个 region；</span><br></pre></td></tr></table></figure>
<p>因为 Major Compaction 存在 write amplification 的问题，所以 major compaction 一般都<strong>手动执行</strong>，安排在写入量的低峰期（例如周末和半夜）。Major compaction 还能将因为 RegionServer crash 或者 Balance 导致的数据迁移重新移回到与 RegionServer 相同的节点，这样就能恢复 <strong>data locality</strong>。</p>
<blockquote>
<p><a href="https://xie.infoq.cn/article/cca8d99fb8ba046e892bc93a0" target="_blank" rel="noopener">LSM树读写放大问题及KV分离技术解析_InfoQ写作社区</a></p>
<ul>
<li>LSM 树的读放大主要来源于读操作需要从 C0~Ck（自顶向下）一层一层查找，直到找到目标数据。这个过程可能需要不止一次 I/O，特别是对于范围查询，读放大非常明显。</li>
<li>采用 LSM 树思想的 KV 数据库的实现中，通常需要启用后台线程周期检查或者手动 flush 等方式触发 Compaction 来减少读放大（减少 SSTable 文件数量）和空间放大（清理过期数据），但也因此带来了严重的<strong>写放大</strong>问题。</li>
</ul>
</blockquote>
<h2 id="查询流程"><a href="#查询流程" class="headerlink" title="查询流程"></a>查询流程</h2><ul>
<li><p>（1）<strong>查询 meta 表</strong>：Client 发送请求给 Zk, 获取 hbase:meta 表在哪个 RegionServer，hbase:meta 存储了每个表在每个 Region 上的 start/end Key， Client 向该 RegionServer 发送请求, 查询 meta 表, 获取 Key 在哪个 Region 上, 同时也确定了 RegionServer</p>
<ul>
<li>查询 hbase:meta 表的步骤可以在 Client 本地进行缓存, 不必每次都去查 Zk；</li>
</ul>
</li>
<li><p>（2）<strong>从 RegionServer 进行合并读</strong>：</p>
<ul>
<li>尝试从 BlockCache 查询（最近读取的 KeyValue 都被缓存在这里，这是 一个 LRU 缓存）</li>
<li>尝试从 MemStore 查询（即写缓存，包含了最近更新的数据）</li>
<li>从 StoreFile（HFile） 查询： 尝试从内存中已加载的 Data Block <strong>索引</strong> 和 <strong>布隆过滤器</strong> 中查询</li>
<li>找到 Rowkey 对应的 Cell</li>
</ul>
</li>
</ul>
<blockquote>
<p>每个列族的 MemStore 可能对应多个 HFile，所以一次查询可能会需要读取多个 HFile，这被称为<strong>读放大</strong>（<strong>read amplification</strong>），尤其是不满足 data locality 时（Region Server 和 HFile 在不同的节点），读放大带来的影响会更严重</p>
</blockquote>
<p>@ref:: </p>
<ul>
<li><a href="http://hbasefly.com/2016/12/21/hbase-getorscan/" target="_blank" rel="noopener">HBase原理－数据读取流程解析 – 有态度的HBase/Spark/BigData</a></li>
<li><a href="http://hbasefly.com/2017/06/11/hbase-scan-2/" target="_blank" rel="noopener">HBase原理－迟到的‘数据读取流程’部分细节 – 有态度的HBase/Spark/BigData</a></li>
<li><a href="http://hbasefly.com/2016/04/03/hbase_hfile_index/" target="_blank" rel="noopener">HBase – 探索HFile索引机制 – 有态度的HBase/Spark/BigData</a></li>
</ul>
<h2 id="写入流程"><a href="#写入流程" class="headerlink" title="写入流程"></a>写入流程</h2><ul>
<li>（1）Region Server 定位过程与度读流程类似；</li>
<li>（2）put 请求到 Region Server ，数据被写入 WAL 后，会被加入到 MemStore 即写缓存。然后服务端就可以向客户端返回 ack 表示写数据完成。<ul>
<li>每个 RegionServer 一般只有一个 WAL，不同子表、不同列族的数据都被写入这一个 WAL, 同时 WAL 中每个最小单元都有唯一递增的seqId @ref: [[#HLog (WAL)]]</li>
<li>每个 Column Family 都有一个 MemStore，不同的列族被写入其对应的 MemStore，在 MemStore 中按 KeyValue 进行排序（见MemStore）；</li>
<li>当某个 Column Family 的 MemStore 中，累积了足够多的的数据后，整个有序数据集就会被写入一个新的 HFile 文件到 HDFS；</li>
<li>当 HFile 越来越多，会触发 Compact 合并操作，把过多的 HFile 合并成一个大的 HFile;</li>
<li>当 Region 越来越大，达到阈值后，还会触发 Split；</li>
</ul>
</li>
</ul>
<p>@ref:</p>
<ul>
<li><a href="https://bbs.huaweicloud.com/blogs/132194" target="_blank" rel="noopener">一条数据的HBase之旅，简明HBase入门教程-Write全流程-云社区-华为云</a></li>
</ul>
<h2 id="高可用-amp-宕机恢复"><a href="#高可用-amp-宕机恢复" class="headerlink" title="高可用 &amp; 宕机恢复"></a>高可用 &amp; 宕机恢复</h2><p>HBase 的高可用是通过 Zookeeper 实现，见「Zookeeper」一节：</p>
<ul>
<li>Master 的宕机发现 &amp; 选主；</li>
<li>Region Server 宕机发现；</li>
</ul>
<p>HDFS 的多副本机制保证 HFile 在不同的 Region Server 上存在冗余副本；</p>
<p><strong>Region Server 宕机恢复：</strong></p>
<ul>
<li>Region Server 因某些原因宕机，与 Zk 失去心跳，超时后临时节点被移除；</li>
<li>HMaster 检测到 Region Server 临时节点被移除，开始执行宕机恢复，切分 Region Server 的 HLog（WAL），分发给其他 Region Server 进行回放。HFile 中保存了该 File 中最大的 seqId，所以只需要找到宕机 RS 上 HFile 最大的 seqId，回放大于该 seqId 的日志记录；</li>
</ul>
<p>HLog 的切分和回放，有如下几种策略，LogSplitting、Distributed Log Splitting、Distributed Log Replay</p>
<p>（1）LogSplitting 策略，HLog 的切分和落盘完全由 HMaster 完成：</p>
<ul>
<li>HMaster 启动一个读线程依次顺序读出每个 HLog 中所有 <code>&lt;HLogKey,WALEdit&gt;</code> 数据对，根据 HLogKey 中包含的 Region 字段，将不同的 Region 数据写入不同的内存 buffer 中，HLog 一节已经介绍过，HLog 文件可以有多个，且当前所有 Region 子表的写入，都落到一个 HLog 文件中；</li>
<li>HMaster 为每个 buffer 会对应启动一个写线程，负责将 buffer 中的数据写入 hdfs 中，每个 hdfs 文件对应一个 Region（子表），然后把 hdfs 文件分配给存活的 Region Server 进行回放；</li>
</ul>
<p>这种日志切分可以完成最基本的任务，但是只有 HMaster 进行 HLog 的切分和落盘，在某些场景下（集群整体宕机）进行恢复可能需要 N 个小时！也因为恢复效率太差，所以开发了 Distributed Log Splitting 策略</p>
<p>（2）Distributed Log Splitting 策略，利用了所有 Region Server 的算力对 HLog 进行切分，效率更高</p>
<ul>
<li>Master 会将待切分日志路径发布到 Zookeeper 节点上（/hbase/splitWAL），每个日志作为一个任务，每个任务都会有对应状态，起始状态为 TASK_UNASSIGNED；</li>
<li>所有 RegionServer 启动之后都注册在这个节点上等待新任务，一旦 Master 发布任务之后，RegionServer 就会抢占该任务；</li>
<li>RegionServer 抢占任务成功之后，会分发给 hlogsplitter 线程切分处理，切分策略同上，也是将 HLog 中的数据按 Region 分类，写入不同的 hdfs 文件中；</li>
<li>Region Server 被分配到不同的 hdfs 文件（对应不同的 Region）,进行回放；</li>
</ul>
<p>Distributed Log Splitting 策略利用 RS 的算力加快故障恢复进程，可以将故障恢复时间降到分钟级别。但会产生很多小文件。小文件数量 = HLog 文件数量 x 宕机 RS 上 Region 个数，例如一台 region server 上有200个 region，90个 hlog 文件。恢复过程会在 hdfs 上创建18000个小文件。</p>
<p>（3）Distributed Log Replay: 在（2）文件的写入，而是读取出数据后直接进行回放</p>
<p>@doubt 遗留问题：</p>
<ul>
<li>宕机恢复过程中，meta 表如何更新？</li>
<li>Distributed Log Replay 策略中，RS 抢到某个 HLog 的切分任务，切分后并没有落盘，那么这个 RS 要独自承担这个 HLog 文件中所有 region 的回放？</li>
</ul>
<hr>
<p>@ref:: </p>
<ul>
<li><a href="http://hbasefly.com/2016/10/29/hbase-regionserver-recovering/" target="_blank" rel="noopener">HBase原理－RegionServer宕机数据恢复 – 有态度的HBase/Spark/BigData</a></li>
<li><a href="https://segmentfault.com/a/1190000037554205" target="_blank" rel="noopener">HBase宕机恢复 - 个人文章 - SegmentFault 思否</a></li>
</ul>
<h2 id="Region-拆分"><a href="#Region-拆分" class="headerlink" title="Region 拆分"></a>Region 拆分</h2><p>HBase 中，一张表由多个子表(regions)组成，这些 regions 分布在多个 Region Server 上面，如果一个表有多个列族，那么每个 region 还分为多个 store，每个 store 对应一个列族。</p>
<p>一旦 Region 的负载过大或者超过阈值时，它就会被分裂成两个新的 Region。Region 的拆分分为自动拆分和手动拆分。自动拆分可以采用不同的策略。</p>
<p>Region 的拆分过程是由 Region Server 完成的，其拆分流程如下：</p>
<ol>
<li>将需要拆分的 Region 下线，客户端对该 Region 的请求会被拒绝，Master 会检测到 Region 的状态为 SPLITTING；</li>
<li>开始拆分 Region，原 Region 被均分为2个子 Region；</li>
<li>完成子 Region 创建后，向 Meta 表发送新产生的 Region 的元数据信息；</li>
<li>将 Region 的拆分信息更新到 HMaster，并且每个 Region 进入可用状态；</li>
</ol>
<p>@doubt： 拆分后的子 Region ，还是在原来的 RS 上提供服务，拆分并不能起到降低 RS 负载的作用？</p>
<p>Region 的 <strong>自动拆分</strong>主要根据拆分策略进行，Region 的拆分逻辑是通过 CompactSplitThread 线程的 requestSplit 方法来触发的，每当执行 Memstore Flush 操作时都会调用该方法进行判断，看是否有必要对目标 Region 进行拆分。</p>
<p>➤ 自动拆分的触发有三种策略：</p>
<ul>
<li><strong>ConstantSizeRegionSplitPolicy</strong>：在0.94之前只有这个策略。当 region 中的一个 store（对应一个 columnfamily 的一个 storefile）超过了配置参数 <code>hbase.hregion.max.filesize</code> 时拆分成两个，该配置参数默认为10GB。region 拆分线是最大 storefile 的中间 rowkey。从字面意思来看，当 region 大小大于某个阈值（hbase.hregion.max.filesize）之后就会触发切分，实际上并不是这样，真正实现中这个阈值是对于某个 store （列族）来说的，即<strong>一个 region 最大的 store 的大小</strong> 大于设置阈值之后才会触发切分。</li>
</ul>
<blockquote>
<p>该策略不是很灵活，如果参数设置的过大，对于写入量大的表可能会触发拆分，但小表在极端情形下，可能永远都无法达到这个阈值</p>
</blockquote>
<ul>
<li><strong>IncreasingToUpperBoundRegionSplitPolicy</strong> 0.94版本~2.0版本默认切分策略。总体来看和 ConstantSizeRegionSplitPolicy 思路相同，一个 region 中最大 store 大小大于设置阈值就会触发切分。但是这个阈值并不是固定值，而是会在一定条件下不断调整，调整规则和 region 所属表在当前 regionserver 上的 region 个数有关系. 当然阈值并不会无限增大，最大值为用户设置的 MaxRegionFileSize。</li>
</ul>
<ul>
<li><strong>SteppingSplitPolicy</strong>: 2.0版本默认切分策略。这种切分策略的切分阈值又发生了变化，相比 IncreasingToUpperBoundRegionSplitPolicy 简单了一些，依然和待分裂 region 所属表在当前 regionserver 上的 region 个数有关系，如果 region 个数等于1，切分阈值为 flush size x 2，否则为 MaxRegionFileSize。</li>
</ul>
<blockquote>
<p>这种切分策略对于大集群中的大表、小表会比 IncreasingToUpperBoundRegionSplitPolicy 更加友好，小表不会再产生大量的小 region，而是适可而止。</p>
</blockquote>
<p>➤ 手动拆分：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">split &apos;regionName&apos; # format: &apos;tableName,startKey,id&apos;</span><br></pre></td></tr></table></figure>
<hr>
<p>@ref:</p>
<ul>
<li><a href="http://hbasefly.com/2017/08/27/hbase-split/" target="_blank" rel="noopener">HBase原理 – 所有Region切分的细节都在这里了 – 有态度的HBase/Spark/BigData</a></li>
</ul>
<h2 id="Region-合并"><a href="#Region-合并" class="headerlink" title="Region 合并"></a>Region 合并</h2><p>目前只有手动合并，当删除大量数据后，HBase 中会存在数量很多的小 Region，MemStore 的数量也会变多，数据频繁从内存 Flush 到 HFile，影响用户请求，可能阻塞该 Region 服务器上的更新操作。</p>
<p><strong>合并过程</strong></p>
<ol>
<li>客户端发起 Region 合并处理，并发送 Region 合并请求给 Master。</li>
<li>Master 在 Region 服务器上把 Region 移到一起，并发起一个 Region 合并操作的请求。</li>
<li>Region 服务器将准备合并的 Region下线，然后进行合并。</li>
<li>从 Meta 表删除被合并的 Region 元数据，新的合并了的 Region 的元数据被更新写入 Meta 表中。</li>
<li>合并的 Region 被设置为上线状态并接受访问，同时更新 Region 信息到 Master。</li>
</ol>
<h2 id="Region-负载均衡"><a href="#Region-负载均衡" class="headerlink" title="Region 负载均衡"></a>Region 负载均衡</h2><p>当 Region 进行拆分之后，Region Server 之间可能会出现 Region 数量 不均衡的问题，Master 便会执行负载均衡来调整部分 Region 的位置，使每个 Region 服务器的 Region 数量保持在合理范围之内，负载均衡会引起 Region 的重新定位，使涉及的 Region 不具备数据本地性。</p>
<p>Region 的负载均衡由 Master 来完成，Master 有一个内置的负载均衡器，在默认情况下，均衡器每 5 分钟运行一次，用户可以配置。负载均衡操作分为两步进行：首先生成负载均衡计划表， 然后按照计划表执行 Region 的分配。</p>
<p>执行负载均衡前要明确，在以下几种情况时，Master 是不会执行负载均衡的。</p>
<ul>
<li>均衡负载开关关闭。</li>
<li>Master 没有初始化。</li>
<li>当前有 Region 处于拆分状态。</li>
<li>当前集群中有 Region 服务器出现故障。</li>
</ul>
<p>Master 内部使用一套集群负载评分的算法，来评估 HBase 某一个表的 Region 是否需要进行重新分配。这套算法分别从 Region 服务器中 Region 的数目、表的 Region 数、MenStore 大小、 StoreFile 大小、数据本地性等几个维度来对集群进行评分，评分越低代表集群的负载越合理。</p>
<p>确定需要负载均衡后，再根据不同策略选择 Region 进行分配，负载均衡策略有三种，如下表所示。</p>
<table>
<thead>
<tr>
<th>策略</th>
<th>原理</th>
</tr>
</thead>
<tbody>
<tr>
<td>RandomRegionPicker</td>
<td>随机选出两个 Region 服务器下的 Region 进行交换</td>
</tr>
<tr>
<td>LoadPicker</td>
<td>获取 Region 数目最多或最少的两个 Region 服务器，使两个 Region 服务器最终的 Region 数目更加平均</td>
</tr>
<tr>
<td>LocalityBasedPicker</td>
<td>选择本地性最强的 Region</td>
</tr>
</tbody>
</table>
<p>根据上述策略选择分配 Region 后再继续对整个表的所有 Region 进行评分，如果依然未达到标准，循环执行上述操作直至整个集群达到负载均衡的状态。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/大数据/" rel="tag"># 大数据</a>
          
            <a href="/tags/数据库-HBase/" rel="tag"># 数据库/HBase</a>
          
            <a href="/tags/数据库-KV/" rel="tag"># 数据库/KV</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/32.Database/HBase-02运维和部署/" rel="next" title="HBase-02运维和部署">
                <i class="fa fa-chevron-left"></i> HBase-02运维和部署
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/32.Database/HBase-04行键设计/" rel="prev" title="HBase-04行键(Rowkey)设计">
                HBase-04行键(Rowkey)设计 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/hexo_avatar.png"
                alt="beefyheisenberg" />
            
              <p class="site-author-name" itemprop="name">beefyheisenberg</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">485</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">34</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">501</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://www.facebook.com/yourname" target="_blank" title="Facebook">
                      
                        <i class="fa fa-fw fa-facebook"></i></a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://twitter.com/" target="_blank" title="Twitter">
                      
                        <i class="fa fa-fw fa-twitter"></i></a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.instagram.com/_kongyang/" target="_blank" title="Instagram">
                      
                        <i class="fa fa-fw fa-instagram"></i></a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i></a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#HBase-架构简介"><span class="nav-text">HBase 架构简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HDFS"><span class="nav-text">HDFS</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Zookeeper"><span class="nav-text">Zookeeper</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#META-表"><span class="nav-text">META 表</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HMaster"><span class="nav-text">HMaster</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HRegion-Server"><span class="nav-text">HRegion Server</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HLog-WAL"><span class="nav-text">HLog (WAL)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MemStore"><span class="nav-text">MemStore</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HFile"><span class="nav-text">HFile</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#V1-V2的改进"><span class="nav-text">V1-V2的改进</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-Block-的三层索引"><span class="nav-text">Data Block 的三层索引</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-Block-的-KV-结构"><span class="nav-text">Data Block 的 KV 结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BloomFilter"><span class="nav-text">BloomFilter</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Minor-Major-Compaction"><span class="nav-text">Minor/Major Compaction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#查询流程"><span class="nav-text">查询流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#写入流程"><span class="nav-text">写入流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#高可用-amp-宕机恢复"><span class="nav-text">高可用 &amp; 宕机恢复</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Region-拆分"><span class="nav-text">Region 拆分</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Region-合并"><span class="nav-text">Region 合并</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Region-负载均衡"><span class="nav-text">Region 负载均衡</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2014 &mdash; <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">beefyheisenberg</span>

  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'https://beefyheisenberg.github.io/32.Database/HBase-03架构和原理/';
          this.page.identifier = '32.Database/HBase-03架构和原理/';
          this.page.title = 'HBase-03架构和原理';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://dropnotes-2.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  
  <script type="text/javascript" src="/js/src/js.cookie.js?v=5.1.4"></script>
  <script type="text/javascript" src="/js/src/scroll-cookie.js?v=5.1.4"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


  

</body>
</html>
